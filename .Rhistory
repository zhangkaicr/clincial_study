library(naniar)
library(UpSetR)
#以下读入文件并进行预览
#以下设定工作目录
setwd("D:/pipeline")
#文件放入工作目录内并进行读取
rawdata <- read.csv("3.csv",header = T,
row.names = ,na.strings = "na")
str(rawdata)#注意变量类型
##输出变量名称及转换为因子变量类型
##以下将需部分变量变为分类变量
vars <- c("ASA")
rawdata<- rawdata %>%
mutate(across(one_of(vars), as.factor))
str(rawdata)
##更改变量因子水平
rawdata$PBD <- relevel(rawdata$PBD,ref = "none")
##查看没有缺失值的行的总体数据集
completa_data<- (rawdata[complete.cases(rawdata),])
##以下查看数据集中缺失值的情况
###以下绘制缺失数据热图
vis_miss(rawdata,
show_perc = TRUE,
show_perc_col = TRUE,
cluster = TRUE)
###以下绘制缺失数据upset图，绘制前10个最多缺失的列
gg_miss_upset(rawdata,nsets = 15)
###一线按照比例进行缺失列内容的删除
na.per<- (dim(rawdata)[1]/1000)###定义每列删除缺失的阈值
###定义删除缺失值函数为naf
naf<-function(x){
nas<-sum(is.na(x))
return(nas)
}
###以下按照缺失值阈值进行筛选
na_names<- apply(rawdata,2, naf)%>%
as.data.frame
newdata<- rawdata[,(na_names[,1]<= na.per)]
以下显示删除的列
colnames(rawdata[,(na_names[,1]>na.per)])
##以下筛选亚组进行保存
#data_pattrn <- subset(newdata,grup=="ablation"&recurrence=="yes")
##以下保存本次处理数据
##newdata为按照阈值删除列后内容
##completa_data为没有缺失的数据集
str(newdata)
newdata$TBIL_team <ifelse(newdata$TBIL>=250,"high","low")
newdata$TBIL_team
str(newdata)
newdata$TBIL
newdata$TBIL_team <-ifelse(newdata$TBIL>=250,"high","low")
newdata$TBIL_team
options(stringsAsFactors = T)
newdata$TBIL_team <-ifelse(newdata$TBIL>=250,"high","low")
newdata$TBIL_team
save(newdata,file = "3newdata.RData")
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = F)
library(compareGroups)
library(rms)
library(survival)
library(survminer)
library(explore)
library(ggpubr)
library(finalfit)
library(condsurv)
#以下读入文件并进行预览
load("3newdata.RData")
str(explore_data)
explore_data <-newdata
str(explore_data)
#以下进行可视化compare操作进行
cGroupsWUI(port = 8102L)
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = F)
library(compareGroups)
library(rms)
library(survival)
library(survminer)
library(explore)
library(ggpubr)
library(finalfit)
library(condsurv)
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(naniar)
library(UpSetR)
#以下读入文件并进行预览
#以下设定工作目录
setwd("D:/pipeline")
rawdata <- read.csv("3.csv",header = T,
row.names = ,na.strings = "na")
rawdata <- read.csv("4.csv",header = T,
row.names = ,na.strings = "na")
str(rawdata)#注意变量类型
##以下将需部分变量变为分类变量
vars <- c("ASA")
##更改变量因子水平
rawdata$PBD <- relevel(rawdata$PBD,ref = "none")
rawdata$PBD
##查看没有缺失值的行的总体数据集
completa_data<- (rawdata[complete.cases(rawdata),])
###以下绘制缺失数据热图
vis_miss(rawdata,
show_perc = TRUE,
show_perc_col = TRUE,
cluster = TRUE)
###以下绘制缺失数据upset图，绘制前10个最多缺失的列
gg_miss_upset(rawdata,nsets = 15)
na.per<- (dim(rawdata)[1]/1000)###定义每列删除缺失的阈值
###定义删除缺失值函数为naf
naf<-function(x){
nas<-sum(is.na(x))
return(nas)
}
###以下按照缺失值阈值进行筛选
na_names<- apply(rawdata,2, naf)%>%
as.data.frame
newdata<- rawdata[,(na_names[,1]<= na.per)]
colnames(rawdata[,(na_names[,1]>na.per)])
save(newdata,file = "3newdata.RData")
save(newdata,file = "4newdata.RData")
library(forestmodel)
library(rms)
library(ggpubr)
library(finalfit)
library(gtsummary)
library(tidyverse)
library(QualInt)
library(eoffice)
#清空环境变量
rm(list = ls())
options(stringsAsFactors = T)
#以下读入文件并进行预览
setwd("D:/pipeline")
load("newdata.RData")
#清空环境变量
rm(list = ls())
options(stringsAsFactors = T)
#以下读入文件并进行预览
setwd("D:/pipeline")
load("newdata.RData")
load("4newdata.RData")
data_final <-newdata
str(data_final)
names(data_final)
#以下进行亚组分析
data_final%>%filter(.,TBIL>=250)->data_final
#以下将所有列名纳入一个向量并选择自变量
names(data_final)%>%dput()->allvar;allvar
allvar <- allvar[1:16];allvar
explanatory <- allvar
names(data_final)
#以下将所有列名纳入一个向量并选择自变量
names(data_final)%>%dput()->allvar;allvar
allvar <- allvar[1:18];allvar
explanatory <- allvar[-5]
explanatory
#以下进行结果变量定义
dependent <-"Complication_Criteria"
#以下进行单因素分析
data_final  %>%
finalfit(dependent,explanatory,
metrics=T,  #metrics=T表示输出模型检验的指标
add_dependent_label=F) -> t2  #add_dependent_label=F表示不在表的左上角添加因变量标签。
#以下进行结果变量定义
dependent <-"Complication_Criteria"
#以下进行单因素分析
data_final  %>%
finalfit(dependent,explanatory,
metrics=T,  #metrics=T表示输出模型检验的指标
add_dependent_label=F) -> t2  #add_dependent_label=F表示不在表的左上角添加因变量标签。
names(data_final)
#以下进行结果变量定义
dependent <-"Complication.Criteria"
#以下进行单因素分析
data_final  %>%
finalfit(dependent,explanatory,
metrics=T,  #metrics=T表示输出模型检验的指标
add_dependent_label=F) -> t2  #add_dependent_label=F表示不在表的左上角添加因变量标签。
write.csv(t2,"Univariate_analysis.csv")
multimodel <- glm(Complication_Criteria ~ WBC + PBD + Hb. + blood_transfusion,
binomial(link="logit"),
data = data_final)
multimodel <- glm(Complication.Criteria ~ WBC + PBD + Hb. + blood_transfusion,
binomial(link="logit"),
data = data_final)
multimodel <- glm(Complication.Criteria ~ WBC + PBD + Hb. + blood.transfusion,
binomial(link="logit"),
data = data_final)
summary(multimodel)
p <- forest_model(multimodel);p
#清空环境变量
rm(list = ls())
options(stringsAsFactors = T)
#以下读入文件并进行预览
setwd("D:/pipeline")
load("4newdata.RData")
data_final <-newdata
str(data_final)
names(data_final)
#以下进行亚组分析
data_final%>%filter(.,TBIL>=250)->data_final
data_final$Hb <- ifelse(data_final$Hb>=90,"normal","low" )
data_final$PD_duration<- ifelse(data_final$PD_duration>=300,"long","short" )
data_final$Hb <- as.factor(data_final$Hb)
data_final$PD_duration <- as.factor(data_final$PD_duration)
data_final <-newdata
str(data_final)
names(data_final)
#清空环境变量
rm(list = ls())
options(stringsAsFactors = T)
#以下读入文件并进行预览
setwd("D:/pipeline")
load("4newdata.RData")
data_final <-newdata
str(data_final)
names(data_final)
#以下进行亚组分析
data_final%>%filter(.,TBIL>=250)->data_final
data_final$Hb. <- ifelse(data_final$Hb. >=90,"normal","low" )
data_final$Hb.
data_final$PD.duration<- ifelse(data_final$PD.duration >=300,"long","short" )
data_final$Hb <- as.factor(data_final$Hb)
data_final$PD_duration <- as.factor(data_final$PD_duration)
#清空环境变量
rm(list = ls())
options(stringsAsFactors = T)
#以下读入文件并进行预览
setwd("D:/pipeline")
load("4newdata.RData")
data_final <-newdata
str(data_final)
names(data_final)
#以下进行亚组分析
data_final%>%filter(.,TBIL>=250)->data_final
data_final$Hb. <- ifelse(data_final$Hb. >=90,"normal","low" )
data_final$PD.duration<- ifelse(data_final$PD.duration >=300,"long","short" )
data_final$Hb. <- as.factor(data_final$Hb.)
data_final$PD.duration<- as.factor(data_final$PD.duration)
#以下将所有列名纳入一个向量并选择自变量
names(data_final)%>%dput()->allvar;allvar
allvar <- allvar[1:18];allvar
explanatory <- allvar[-5]
#以下进行结果变量定义
dependent <-"Complication.Criteria"
#以下进行单因素分析
data_final  %>%
finalfit(dependent,explanatory,
metrics=T,  #metrics=T表示输出模型检验的指标
add_dependent_label=F) -> t2  #add_dependent_label=F表示不在表的左上角添加因变量标签。
write.csv(t2,"Univariate_analysis.csv")
write.csv(t2,"Univariate_analysis.csv")
explanatory <- allvar[-10]
#以下进行结果变量定义
dependent <-"Complication.Criteria"
#以下进行单因素分析
data_final  %>%
finalfit(dependent,explanatory,
metrics=T,  #metrics=T表示输出模型检验的指标
add_dependent_label=F) -> t2  #add_dependent_label=F表示不在表的左上角添加因变量标签。
write.csv(t2,"1Univariate_analysis.csv")
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(MatchIt)
library(compareGroups)
下中文内容应用UTF8编码，如无法正常显示可以open with encoding
#本部分为数据预处理，将进行数据载入及清洗
##数据载入，请把文件名放在与代码脚本同一个文件夹下
##文件夹请用英文命名
##读入的文件请转换为csv结尾的文件
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(naniar)
library(UpSetR)
#以下读入文件并进行预览
#以下设定工作目录
setwd("D:/pipeline")
#文件放入工作目录内并进行读取
rawdata <- read.csv("3.csv",header = T,
row.names = ,na.strings = "na")
str(rawdata)#注意变量类型
##输出变量名称及转换为因子变量类型
##以下将需部分变量变为分类变量
vars <- c("ASA")
rawdata<- rawdata %>%
mutate(across(one_of(vars), as.factor))
str(rawdata)
##更改变量因子水平
rawdata$PBD <- relevel(rawdata$PBD,ref = "none")
##查看没有缺失值的行的总体数据集
completa_data<- (rawdata[complete.cases(rawdata),])
##以下查看数据集中缺失值的情况
###以下绘制缺失数据热图
vis_miss(rawdata,
show_perc = TRUE,
show_perc_col = TRUE,
cluster = TRUE)
###以下绘制缺失数据upset图，绘制前10个最多缺失的列
gg_miss_upset(rawdata,nsets = 15)
###一线按照比例进行缺失列内容的删除
na.per<- (dim(rawdata)[1]/1000)###定义每列删除缺失的阈值
###定义删除缺失值函数为naf
naf<-function(x){
nas<-sum(is.na(x))
return(nas)
}
###以下按照缺失值阈值进行筛选
na_names<- apply(rawdata,2, naf)%>%
as.data.frame
newdata<- rawdata[,(na_names[,1]<= na.per)]
以下显示删除的列
colnames(rawdata[,(na_names[,1]>na.per)])
##以下筛选亚组进行保存
#data_pattrn <- subset(newdata,grup=="ablation"&recurrence=="yes")
##以下保存本次处理数据
##newdata为按照阈值删除列后内容
##completa_data为没有缺失的数据集
str(newdata)
save(newdata,file = "3newdata.RData")
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(MatchIt)
library(compareGroups)
###以下读入文件并进行预览
load("3newdata.RData")
prematch_data <- newdata#定义后续探索数据的范围
str(prematch_data)
table(prematch_data$grup)#查看分组变量的分布情况
table(prematch_data$PBD)#查看分组变量的分布情况
prematch_data <- mutate(prematch_data,PBD = case_when(
PBD == "yes"~ 1 ,
PBD == "none" ~ 0 ))
View(prematch_data)
str(prematch_data)
#以下进行PSM分析，获得的分析结果为一个list
psm<- matchit(data = prematch_data,#数据集
formula = PBD ~ age+BMI+ASA+ALB+Hb+blood.transfusion,
#以上为PSM方程，~  左侧为分组变量，右侧为待匹配变量
method = "nearest",#此获得PSM评分后，应用什么方法进行对照组的匹配
distance = "logit",#计算PSM评分的方法
replace = FALSE,#是否进行有放回的匹配
caliper = 0.2,#卡钳值，最终选择的两组的PSM评分的差距
ratio = 1)#匹配的比例，一般为1:1到1:4
match.data(psm)#显示匹配后的数据，
#distance为每个对象的倾向性得分，
#weights为每个对象的权重。
summary(psm)#查看匹配的结构
#以下进行PSM分析，获得的分析结果为一个list
psm<- matchit(data = prematch_data,#数据集
formula = PBD ~ age+BMI+ASA+ALB+Hb+blood.transfusion,
#以上为PSM方程，~  左侧为分组变量，右侧为待匹配变量
method = "nearest",#此获得PSM评分后，应用什么方法进行对照组的匹配
distance = "logit",#计算PSM评分的方法
replace = FALSE,#是否进行有放回的匹配
caliper = 0.05,#卡钳值，最终选择的两组的PSM评分的差距
ratio = 1)#匹配的比例，一般为1:1到1:4
match.data(psm)#显示匹配后的数据，
#distance为每个对象的倾向性得分，
#weights为每个对象的权重。
summary(psm)#查看匹配的结构
plot(psm,
type="histogram",
interactive = F)  #使用直条图展示匹配前后倾向性评分分布
colnames(data_matched)
data_matched<-match.data(psm)#提取匹配后的数据集
Non_result<- descrTable(PBD ~., method = 2,data = data_matched)#对比两组的基线情况，连续变量应用非参数检验
Non_result
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(MatchIt)
library(compareGroups)
###以下读入文件并进行预览
load("3newdata.RData")
prematch_data <- newdata#定义后续探索数据的范围
str(prematch_data)
prematch_data <- subset(prematch_data,prematch_data$TBIL>=250)
prematch_data <- mutate(prematch_data,PBD = case_when(
PBD == "yes"~ 1 ,
PBD == "none" ~ 0 ))
str(prematch_data)
#以下进行PSM分析，获得的分析结果为一个list
psm<- matchit(data = prematch_data,#数据集
formula = PBD ~ age+BMI+ASA+ALB+Hb+blood.transfusion,
#以上为PSM方程，~  左侧为分组变量，右侧为待匹配变量
method = "nearest",#此获得PSM评分后，应用什么方法进行对照组的匹配
distance = "logit",#计算PSM评分的方法
replace = FALSE,#是否进行有放回的匹配
caliper = 0.2,#卡钳值，最终选择的两组的PSM评分的差距
ratio = 1)#匹配的比例，一般为1:1到1:4
match.data(psm)#显示匹配后的数据，
#distance为每个对象的倾向性得分，
#weights为每个对象的权重。
summary(psm)#查看匹配的结构
plot(psm,
type="histogram",
interactive = F)  #使用直条图展示匹配前后倾向性评分分布
colnames(data_matched)
Non_result<- descrTable(PBD ~., method = 2,data = data_matched)#对比两组的基线情况，连续变量应用非参数检验
data_matched<-match.data(psm)#提取匹配后的数据集
colnames(data_matched)
Non_result<- descrTable(PBD ~., method = 2,data = data_matched)#对比两组的基线情况，连续变量应用非参数检验
Non_result
#以下导出结果
save(data_matched,file =  "data_matched.RData" )
export2word(Non_result,file = "匹配后基线信息对比表.docx" )
library("cobalt")
install.packages("cobalt")
library("cobalt")
library("MatchIt")
data("lalonde", package = "cobalt")
data("lalonde", package = "cobalt")
force(lalonde)
View(lalonde)
# Nearest neighbor matching with MatchIt
m.out <- matchit(treat ~ age + educ + race + married + nodegree + re74 + re75, data = lalonde)
# Checking balance before and after matching:
bal.tab(m.out, thresholds = c(m = 0.1), un = TRUE)
#> Variable with the greatest mean difference
#>    Variable Diff.Adj        M.Threshold
#>  race_black    0.373 Not Balanced, >0.1
#>
#> Sample sizes
#>           Control Treated
#> All           429     185
#> Matched       185     185
#> Unmatched     244       0
# Examining distributional balance with plots:
bal.plot(m.out, var.name = "educ")
bal.plot(m.out, var.name = "distance", mirror = TRUE, type = "histogram")
# Generating a Love plot to report balance:
love.plot(m.out, stats = c("mean.diffs", "variance.ratios"), thresholds = c(m = 0.1,
v = 2), abs = TRUE, binary = "std", var.order = "unadjusted")
# Generating a Love plot to report balance:
love.plot(m.out, stats = c("mean.diffs"), thresholds = c(m = 0.1,
v = 2), abs = TRUE, binary = "std", var.order = "unadjusted")
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(MatchIt)
library(compareGroups)
###以下读入文件并进行预览
load("3newdata.RData")
prematch_data <- newdata#定义后续探索数据的范围
str(prematch_data)
table(prematch_data$PBD)#查看分组变量的分布情况
prematch_data <- subset(prematch_data,prematch_data$TBIL>=250)
prematch_data <- mutate(prematch_data,PBD = case_when(
PBD == "yes"~ 1 ,
PBD == "none" ~ 0 ))
str(prematch_data)
#以下进行PSM分析，获得的分析结果为一个list
psm<- matchit(data = prematch_data,#数据集
formula = PBD ~ age+BMI+ASA+ALB+Hb+blood.transfusion,
#以上为PSM方程，~  左侧为分组变量，右侧为待匹配变量
method = "nearest",#此获得PSM评分后，应用什么方法进行对照组的匹配
distance = "logit",#计算PSM评分的方法
replace = FALSE,#是否进行有放回的匹配
caliper = 0.2,#卡钳值，最终选择的两组的PSM评分的差距
ratio = 1)#匹配的比例，一般为1:1到1:4
match.data(psm)#显示匹配后的数据，
#distance为每个对象的倾向性得分，
#weights为每个对象的权重。
summary(psm)#查看匹配的结构
plot(psm,
type="histogram",
interactive = F)  #使用直条图展示匹配前后倾向性评分分布
data_matched<-match.data(psm)#提取匹配后的数据集
love.plot(psm, stats = c("mean.diffs"), thresholds = c(m = 0.1,
v = 2), abs = TRUE, binary = "std", var.order = "unadjusted")
# all defaults
deepres <- deepsurv(data = data_train,
time_variable = "OS",
status_variable ="status",
frac = 0.3,
activation = "relu",
num_nodes = c(4L, 8L, 4L, 2L),
dropout = 0.1,
reverse=F,
early_stopping = TRUE,
epochs = 10L,
batch_size = 32L)
source('C:/Users/85330/Desktop/clincial_study/clincial_study/6_deep_survival.R', encoding = 'UTF-8')
remotes::install_github("ehrlinger/ggRandomForests")
##载入需要的R包
rm(list = ls())
options(stringsAsFactors = T)
library(tidyverse)
library(naniar)
library(UpSetR)
install.packages("E:/ggdownload/ggRandomForests-master.zip", repos = NULL, type = "win.binary")
library(ggRandomForests)
install.packages("E:/ggdownload/ggRandomForests-master.zip", repos = NULL, type = "win.binary")
library(`ggRandomForests-master`)
library(`ggRandomForests`)
library(ggRandomForests)
library(ggDCA)
library(`ggRandomForests-master`)
